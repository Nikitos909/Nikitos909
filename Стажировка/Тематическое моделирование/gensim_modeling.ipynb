{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Библиотека gensim для тематического моделирования\n",
    "## Данная модель основана на байесовских методов машинного обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C://Users/DeryabinNS/Articles')\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing frequency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"articles.txt\", \"r\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = [re.split('[^a-z]', sentence.lower().replace('\\n', '')) for sentence in text.split()]\n",
    "words = [word for sen in df for word in sen if word != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = []\n",
    "for i in words:\n",
    "    unique_words.append(morph.parse(i)[0].normal_form)\n",
    "words = unique_words\n",
    "unique_words = list(set(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocab.txt\", \"w\") as f:\n",
    "    for k in unique_words:\n",
    "        f.write(k+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix(name_file):\n",
    "    with open(name_file, \"r\") as file:\n",
    "        text = file.read()\n",
    "    df = [re.split('[^a-z]', sentence.lower().replace('\\n', '')) for sentence in text.split()]\n",
    "    words = [word for sen in df for word in sen if word != '']\n",
    "    list_words = []\n",
    "    found = {}\n",
    "    for i in words:\n",
    "        list_words.append(morph.parse(i)[0].normal_form)\n",
    "    for i in list_words:\n",
    "        if i in unique_words:\n",
    "            found.setdefault(unique_words.index(i), 0)\n",
    "            found[unique_words.index(i)] += 1\n",
    "    for i in name_file:\n",
    "        if i.isdigit():\n",
    "            m = i\n",
    "    with open(\"docword.txt\", \"a\") as f:\n",
    "        for k, v in found.items():\n",
    "            f.write(str(m) + ' ' + str(k) + ' ' + str(v) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_articles = ['article1.txt', 'article2.txt', 'article3.txt', 'article4.txt', 'article5.txt']\n",
    "for i in list_articles:\n",
    "    get_matrix(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем данные в формте UCI Bag of Words\n",
    "data = corpora.UciCorpus(\"docword.txt\", \"vocab.txt\")\n",
    "dictionary = data.create_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучение модель\n",
    "ldamodel = models.ldamodel.LdaModel(data, id2word=dictionary, num_topics=6, passes=20, alpha=1.25, eta=1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение модели\n",
    "ldamodel.save(\"ldamodel_xkcd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка модели\n",
    "ldamodel = models.ldamodel.LdaModel.load(\"ldamodel_xkcd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 : 0.001*\"b'finally'\" + 0.001*\"b'intricate'\" + 0.001*\"b'released'\" + 0.001*\"b'attorney'\" + 0.001*\"b'justice'\" + 0.001*\"b'uncertain'\" + 0.001*\"b'wales'\" + 0.001*\"b'dramatic'\" + 0.001*\"b'excelled'\" + 0.001*\"b'assertion'\"\n",
      "Topic 1 : 0.001*\"b'finally'\" + 0.001*\"b'attorney'\" + 0.001*\"b'intricate'\" + 0.001*\"b'justice'\" + 0.001*\"b'released'\" + 0.001*\"b'uncertain'\" + 0.001*\"b'assertion'\" + 0.001*\"b'something'\" + 0.001*\"b'books'\" + 0.001*\"b'credits'\"\n",
      "Topic 2 : 0.026*\"b'finally'\" + 0.015*\"b'uncertain'\" + 0.013*\"b'released'\" + 0.010*\"b'attorney'\" + 0.008*\"b'intricate'\" + 0.008*\"b'justice'\" + 0.006*\"b'signed'\" + 0.006*\"b'start'\" + 0.005*\"b'books'\" + 0.005*\"b'dramatic'\"\n",
      "Topic 3 : 0.019*\"b'finally'\" + 0.009*\"b'attorney'\" + 0.009*\"b'justice'\" + 0.009*\"b'released'\" + 0.008*\"b'intricate'\" + 0.007*\"b'uncertain'\" + 0.006*\"b'house'\" + 0.006*\"b'euro'\" + 0.005*\"b'excelled'\" + 0.005*\"b'broader'\"\n",
      "Topic 4 : 0.001*\"b'finally'\" + 0.001*\"b'uncertain'\" + 0.001*\"b'attorney'\" + 0.001*\"b'intricate'\" + 0.001*\"b'released'\" + 0.001*\"b'justice'\" + 0.001*\"b'wales'\" + 0.001*\"b'assertion'\" + 0.001*\"b'books'\" + 0.001*\"b'something'\"\n",
      "Topic 5 : 0.022*\"b'finally'\" + 0.013*\"b'gen'\" + 0.013*\"b'uncertain'\" + 0.013*\"b'released'\" + 0.012*\"b'intricate'\" + 0.012*\"b'attorney'\" + 0.010*\"b'justice'\" + 0.010*\"b'wales'\" + 0.007*\"b'york'\" + 0.007*\"b'dramatic'\"\n"
     ]
    }
   ],
   "source": [
    "# выводим топы слов\n",
    "for t, top_words in ldamodel.print_topics(num_topics=10, num_words=10):\n",
    "    print(\"Topic\", t, \":\", top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.91525133771681\n"
     ]
    }
   ],
   "source": [
    "# Вычисляем логарифм перплексии и немного преобразуем, чтобы привести к общепринятому виду\n",
    "perplexity = ldamodel.log_perplexity(list(data))\n",
    "print(2**(-perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.9832463)]\n",
      "[(2, 0.9574172), (5, 0.011107269)]\n",
      "[(0, 0.010218977), (1, 0.010218729), (2, 0.015738789), (3, 0.93840563), (4, 0.010218189), (5, 0.015199698)]\n",
      "[(5, 0.99055004)]\n",
      "[(3, 0.96978873)]\n"
     ]
    }
   ],
   "source": [
    "# Получение распределения тем для конкретного документа\n",
    "doc = list(data)\n",
    "for i in range(len(doc)):\n",
    "    d = doc[i]\n",
    "    print(ldamodel.get_document_topics(d))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
